---
layout: post
title: "Deep Learning Optimizer's Prayer"
date: 2025-04-27
---

# The Deep Learning Optimizer's Prayer

That problem is smooth.
And if it's not, it is differentiable everywhere.
And if it's not, we avoid the kinks almost surely.
And if we don't, what is computed is a subgradient.
And if it isn't, it approximates one.
And if that's not true, who cares? The loss went down.

![Animation showing the Frank-Wolfe algorithm](/blog/frank_wolfe.gif?raw=true)